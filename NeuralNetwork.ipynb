{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "#TENSORFLOW ONLY USED FOR THE MNIST DATA SET, NOTHING ELSE\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "when the input z is a vector or Numpy array, Numpy automatically applies the \n",
    "function sigmoid elementwise, that is, in vectorized form.\n",
    "\"\"\"\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this code, the list sizes contains the number of neurons in the respective \n",
    "layers. So, for example, if we want to create a Network object with 2 neurons \n",
    "in the first layer, 3 neurons in the second layer, and 1 neuron in the final \n",
    "layer, we'd do this with the code:\n",
    "\n",
    "net = Network([2, 3, 1])\n",
    "\n",
    "The biases and weights in the Network object are all initialized randomly, using\n",
    "the Numpy np.random.randn function to generate Gaussian distributions with mean \n",
    "0 and standard deviation 1.\n",
    "\n",
    "Assumes first layer is an input layer and omits to set any biases for those \n",
    "neurons\n",
    "\"\"\"\n",
    "class Network(object):\n",
    "\n",
    "  def __init__(self, sizes):\n",
    "    self.num_layers = len(sizes)\n",
    "    self.sizes = sizes\n",
    "    # gives a bias to all neurons except the first layer which are assumed to be\n",
    "    # input\n",
    "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "    # makes y by x array for the weights, zips all nodes except the last layer\n",
    "    # with all nodes except the first layer\n",
    "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "  def feedforward(self, a):\n",
    "    \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "      a = sigmoid(np.dot(w, a)+b)\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "    \"\"\"Train the neural network using batch SGD. 'training_data' is a list of\n",
    "    tuples '(x, y)' representing the training inputs and their labels. If \n",
    "    'test_data' is provided then the network will be evaluated against the test\n",
    "    data after each epoch, with partial progress printed out. Useful for tracking\n",
    "    progress but slows the function down substantially.\n",
    "    \n",
    "    'eta' is learning rate.\n",
    "    'mini_batch_size' is the size of the batches\n",
    "    'epochs' is the amount of epochs to run\n",
    "    \n",
    "    The code works as follows. In each epoch, it starts by randomly shuffling \n",
    "    the training data, and then partitions it into mini-batches of the \n",
    "    appropriate size. This is an easy way of sampling randomly from the training\n",
    "    data. Then for each mini_batch we apply a single step of gradient descent. \n",
    "    This is done by the code 'self.update_mini_batch(mini_batch, eta)', which \n",
    "    updates the network weights and biases according to a single iteration of \n",
    "    gradient descent, using just the training data in mini_batch.\"\"\"\n",
    "    if test_data: n_test = len(test_data)\n",
    "    n = len(training_data)\n",
    "    for j in range(epochs):\n",
    "      random.shuffle(training_data)\n",
    "      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "      for mini_batch in mini_batches:\n",
    "        self.update_mini_batch(mini_batch, eta)\n",
    "      if test_data:\n",
    "        print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "      else:\n",
    "        print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    \n",
    "  def update_mini_batch(self, mini_batch, eta):\n",
    "    \"\"\"Update the network's weights and biases by applying a single step of\n",
    "    gradient descent using backpropagation to a single mini batch for each call\n",
    "    to backprop. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "    is the learning rate.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    for x, y in mini_batch:\n",
    "      # Invokes backpropogation algorithm which is a fast way of computing the \n",
    "      # gradient of the cost function\n",
    "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    \n",
    "    \n",
    "  def backprop(self, x, y):\n",
    "    \"\"\"Return a tuple'(nabla_b, nabla_w)' representing the gradient for the cost\n",
    "    function C_x. 'nabla_b' and 'nabla_w' are layer by layer lists of numpy \n",
    "    arrays, similar to 'self.biases' and 'self.weights'.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    # feedforward\n",
    "    activation = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "      z = np.dot(w, activation)+b\n",
    "      zs.append(z)\n",
    "      activation = sigmoid(z)\n",
    "      activations.append(activation)\n",
    "    # backward pass\n",
    "    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    # Note that the variable l in the loop below is used a little\n",
    "    # differently to the notation in Chapter 2 of the book.  Here,\n",
    "    # l = 1 means the last layer of neurons, l = 2 is the\n",
    "    # second-last layer, and so on.  It's a renumbering of the\n",
    "    # scheme in the book, used here to take advantage of the fact\n",
    "    # that Python can use negative indices in lists.\n",
    "    for l in range(2, self.num_layers):\n",
    "      z = zs[-l]\n",
    "      sp = sigmoid_prime(z)\n",
    "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "      nabla_b[-l] = delta\n",
    "      nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "\n",
    "  def evaluate(self, test_data):\n",
    "    test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "    return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "\n",
    "  def cost_derivative(self, output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x \\partial a for the\n",
    "    output activations.\"\"\"\n",
    "    return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  # Load and prepare the MNIST dataset.\n",
    "  mnist = tf.keras.datasets.mnist\n",
    "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "  # Convert the data from integers to floating-point numbers\n",
    "  x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "  return(x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def load_data_wrapper2():\n",
    "  x_tr, x_ts, y_tr, y_ts = load_data()\n",
    "  x_tr = [np.reshape(x, (784, 1)) for x in x_train[:]]\n",
    "  y_tr = [np.argmax(vectorized_result(y)) for y in y_train[:]]\n",
    "  training_data = list(zip(x_tr, y_tr))\n",
    "  x_ts = [np.reshape(x, (784, 1)) for x in x_test[:]]\n",
    "  y_ts = [np.argmax(vectorized_result(y)) for y in y_test[:]]\n",
    "  test_data = list(zip(x_ts, y_ts))\n",
    "  return(training_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "  x_tr, x_ts, y_tr, y_ts = load_data()\n",
    "  x_tr = [np.reshape(x, (784, 1)) for x in x_train[:]]\n",
    "  y_tr = [vectorized_result(y) for y in y_train[:]]\n",
    "  training_data = list(zip(x_tr, y_tr))\n",
    "  x_ts = [np.reshape(x, (784, 1)) for x in x_test[:]]\n",
    "  y_ts = [vectorized_result(y) for y in y_test[:]]\n",
    "  test_data = list(zip(x_ts, y_ts))\n",
    "  return(training_data, test_data)\n",
    "\n",
    "\n",
    "def vectorized_result(j):\n",
    "  \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "  position and zeroes elsewhere.  This is used to convert a digit\n",
    "  (0...9) into a corresponding desired output from the neural\n",
    "  network.\"\"\"\n",
    "  e = np.zeros((10, 1))\n",
    "  e[j] = 1.0\n",
    "  return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the image: (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16e49f760>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract the first image in x_train\n",
    "idx = 125\n",
    "img = x_train[idx, :, :] # : means that we include all indices\n",
    "print(\"Shape of the image:\", img.shape)\n",
    "plt.imshow(img, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, ts_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.204],\n",
       "        [0.945],\n",
       "        [0.337],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.012],\n",
       "        [0.051],\n",
       "        [0.051],\n",
       "        [0.424],\n",
       "        [0.537],\n",
       "        [0.537],\n",
       "        [0.537],\n",
       "        [0.933],\n",
       "        [0.996],\n",
       "        [0.094],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.475],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.404],\n",
       "        [0.008],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.09 ],\n",
       "        [0.584],\n",
       "        [0.871],\n",
       "        [0.996],\n",
       "        [0.973],\n",
       "        [0.898],\n",
       "        [0.929],\n",
       "        [0.996],\n",
       "        [0.965],\n",
       "        [0.722],\n",
       "        [0.412],\n",
       "        [0.945],\n",
       "        [0.071],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.451],\n",
       "        [0.996],\n",
       "        [0.973],\n",
       "        [0.525],\n",
       "        [0.294],\n",
       "        [0.   ],\n",
       "        [0.357],\n",
       "        [0.996],\n",
       "        [0.357],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.353],\n",
       "        [0.027],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.325],\n",
       "        [0.996],\n",
       "        [0.561],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.247],\n",
       "        [0.922],\n",
       "        [0.878],\n",
       "        [0.145],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.149],\n",
       "        [0.996],\n",
       "        [0.933],\n",
       "        [0.278],\n",
       "        [0.043],\n",
       "        [0.8  ],\n",
       "        [0.804],\n",
       "        [0.067],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.122],\n",
       "        [0.933],\n",
       "        [0.996],\n",
       "        [0.929],\n",
       "        [0.761],\n",
       "        [0.996],\n",
       "        [0.608],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.278],\n",
       "        [0.929],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.698],\n",
       "        [0.063],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.682],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.796],\n",
       "        [0.161],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.133],\n",
       "        [0.922],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.584],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.047],\n",
       "        [0.655],\n",
       "        [0.996],\n",
       "        [0.788],\n",
       "        [0.776],\n",
       "        [0.996],\n",
       "        [0.933],\n",
       "        [0.294],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.306],\n",
       "        [0.996],\n",
       "        [0.839],\n",
       "        [0.059],\n",
       "        [0.047],\n",
       "        [0.804],\n",
       "        [0.996],\n",
       "        [0.937],\n",
       "        [0.137],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.106],\n",
       "        [0.937],\n",
       "        [0.996],\n",
       "        [0.635],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.757],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.169],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.541],\n",
       "        [0.996],\n",
       "        [0.933],\n",
       "        [0.2  ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.757],\n",
       "        [0.996],\n",
       "        [0.882],\n",
       "        [0.106],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.039],\n",
       "        [0.761],\n",
       "        [0.996],\n",
       "        [0.396],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.757],\n",
       "        [0.996],\n",
       "        [0.49 ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.098],\n",
       "        [0.996],\n",
       "        [0.863],\n",
       "        [0.035],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.427],\n",
       "        [0.976],\n",
       "        [0.729],\n",
       "        [0.02 ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.553],\n",
       "        [0.996],\n",
       "        [0.851],\n",
       "        [0.078],\n",
       "        [0.443],\n",
       "        [0.788],\n",
       "        [0.98 ],\n",
       "        [0.996],\n",
       "        [0.49 ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.071],\n",
       "        [0.98 ],\n",
       "        [0.996],\n",
       "        [0.992],\n",
       "        [0.961],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.996],\n",
       "        [0.514],\n",
       "        [0.012],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.004],\n",
       "        [0.357],\n",
       "        [1.   ],\n",
       "        [1.   ],\n",
       "        [1.   ],\n",
       "        [1.   ],\n",
       "        [0.651],\n",
       "        [0.153],\n",
       "        [0.008],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ],\n",
       "        [0.   ]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data[125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 8271 / 10000\n",
      "Epoch 1: 9314 / 10000\n",
      "Epoch 2: 9403 / 10000\n",
      "Epoch 3: 9464 / 10000\n",
      "Epoch 4: 9516 / 10000\n",
      "Epoch 5: 9518 / 10000\n",
      "Epoch 6: 9525 / 10000\n",
      "Epoch 7: 9527 / 10000\n",
      "Epoch 8: 9550 / 10000\n",
      "Epoch 9: 9554 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784, 50, 10])\n",
    "net.fit(tr_data, 10, 10, 3.0, test_data=ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.879,  0.151, -2.478, ..., -0.148,  0.677,  1.367],\n",
      "       [ 0.547, -1.459,  0.67 , ..., -0.01 ,  1.029,  0.353],\n",
      "       [-0.405, -1.009,  1.702, ...,  0.945, -0.199,  1.588],\n",
      "       ...,\n",
      "       [ 0.085, -0.593,  0.46 , ...,  1.29 , -0.549,  1.912],\n",
      "       [ 1.272,  0.442,  0.368, ..., -0.722, -0.513,  0.59 ],\n",
      "       [-0.228,  0.496, -2.584, ...,  0.111,  0.659,  0.191]]), array([[-0.919,  1.983,  0.308, -1.964, -2.353,  0.744, -2.785,  0.702,\n",
      "         0.183,  0.581, -3.416, -1.435,  0.082,  3.836,  1.157,  0.191,\n",
      "        -0.681, -4.231, -0.183, -1.884,  1.19 , -1.982,  2.242, -3.268,\n",
      "         0.775, -1.569, -0.485,  2.816, -2.804, -1.649,  3.005,  2.95 ,\n",
      "         0.401,  1.146, -1.198,  1.984,  1.067, -2.243,  0.546,  2.418,\n",
      "        -2.644, -0.288,  0.898, -2.003,  2.174, -3.387,  0.236, -1.75 ,\n",
      "        -1.017, -2.531],\n",
      "       [-0.764, -1.468,  0.903, -2.481, -3.297, -2.373, -0.401,  1.474,\n",
      "         0.447, -2.801, -0.79 , -1.848,  1.675, -1.406, -2.677,  2.335,\n",
      "         2.059,  1.095, -1.712, -0.78 ,  2.257,  1.309,  0.856, -1.083,\n",
      "        -0.234,  2.668, -0.355,  0.355, -0.195, -0.323, -1.158, -0.406,\n",
      "         1.278, -1.406, -1.173, -2.179, -3.155,  2.462,  2.55 , -2.085,\n",
      "        -3.832,  2.718,  1.354, -0.336,  1.399,  0.993,  0.101,  1.265,\n",
      "        -0.665,  0.766],\n",
      "       [ 0.666, -0.995, -1.133,  0.571, -1.426,  0.467, -4.13 , -0.137,\n",
      "        -3.094,  0.945, -1.855, -3.224, -3.628, -3.045,  1.692,  2.285,\n",
      "         2.831, -2.027,  2.898,  2.376, -1.409, -0.073,  0.611, -3.437,\n",
      "        -0.827, -0.646, -0.677, -0.286, -1.707,  4.78 , -1.198, -3.293,\n",
      "         3.386,  0.188,  2.162,  2.651, -0.24 , -4.754,  0.194,  2.86 ,\n",
      "        -1.614, -1.736,  2.76 , -0.591, -2.073,  1.886, -1.034,  1.973,\n",
      "        -0.481,  1.178],\n",
      "       [ 1.397,  0.685,  0.87 ,  1.597, -3.503,  0.208, -1.335, -0.64 ,\n",
      "         0.61 , -0.459, -2.613, -2.965,  2.967, -2.468,  1.154,  0.585,\n",
      "        -3.701, -3.459, -2.957,  2.618, -0.031, -1.974, -0.177, -5.672,\n",
      "        -0.484,  3.807,  1.492,  2.573,  0.485,  3.12 , -1.771,  0.575,\n",
      "        -2.763,  0.213, -1.806, -1.849, -1.532, -0.376, -1.094, -3.194,\n",
      "         3.062, -0.879, -2.45 , -0.75 , -2.083,  0.641,  1.791, -0.758,\n",
      "        -0.497,  0.506],\n",
      "       [-2.643, -4.569, -0.501,  0.188,  2.133,  1.233,  1.476, -0.797,\n",
      "        -1.221, -0.789, -2.852, -0.924, -0.917,  0.717, -2.028, -0.794,\n",
      "        -1.674,  0.733,  0.066, -0.757, -0.628, -2.188, -0.905, -2.141,\n",
      "        -0.298,  0.605, -2.66 , -1.395,  0.449, -0.725, -2.03 ,  0.061,\n",
      "        -2.765,  1.536,  3.184, -1.209,  0.813,  1.911,  1.458, -0.219,\n",
      "        -2.083,  3.36 , -0.226,  0.269,  2.183, -4.406,  0.829, -2.018,\n",
      "        -4.817, -0.496],\n",
      "       [-1.622,  1.053, -0.756,  1.589, -1.186, -3.705, -1.126, -3.888,\n",
      "        -1.086,  1.   ,  4.853, -2.528,  1.81 , -4.34 , -0.366,  0.034,\n",
      "        -0.812,  1.903,  1.321, -1.404,  2.27 ,  1.375, -1.65 ,  5.407,\n",
      "        -0.301, -4.647,  0.9  ,  7.107,  1.647, -1.984, -0.7  , -2.392,\n",
      "         3.357,  0.145,  0.236, -0.157,  2.142,  2.36 , -1.261, -3.179,\n",
      "        -2.025, -0.89 ,  0.328, -0.065, -2.37 ,  0.197,  0.414,  0.753,\n",
      "         0.441, -1.598],\n",
      "       [ 0.773, -3.224,  1.941, -0.26 , -1.8  ,  3.903, -2.216,  1.402,\n",
      "         0.935, -1.25 ,  2.508, -3.409, -0.801, -3.622, -2.829, -0.775,\n",
      "        -1.221, -0.892, -0.542, -4.055, -1.424,  1.016, -2.535,  1.061,\n",
      "         2.578, -1.3  , -0.707, -0.888,  0.918, -1.844,  2.105,  3.276,\n",
      "        -1.801, -3.121,  1.436,  1.627,  0.418, -0.624,  1.027,  3.246,\n",
      "        -0.498, -0.141, -0.578,  0.864, -0.253,  2.94 ,  0.042, -0.315,\n",
      "        -2.03 , -3.311],\n",
      "       [-0.855,  0.677,  0.777, -2.265,  5.266, -1.703, -1.071,  0.59 ,\n",
      "         1.237,  2.249, -1.323, -0.52 , -1.953, -1.769,  2.651,  1.654,\n",
      "         2.713,  0.402, -3.329,  1.617,  2.525,  1.485, -0.791, -1.671,\n",
      "        -0.576, -0.595, -2.206, -3.552,  2.152,  0.013, -1.168, -1.489,\n",
      "         1.704, -0.546, -1.521,  0.842,  0.44 ,  1.764, -2.372,  1.491,\n",
      "        -0.261, -0.492,  0.009, -3.137, -2.915, -4.277, -0.512, -2.394,\n",
      "        -0.21 ,  0.677],\n",
      "       [ 1.15 ,  0.182, -0.43 , -0.632, -4.895, -3.155, -1.847, -2.466,\n",
      "         1.52 , -0.567, -2.457, -2.945, -3.217, -0.488, -2.129, -4.667,\n",
      "        -2.027,  0.406,  0.239, -3.907, -4.38 , -1.867,  0.674, -3.4  ,\n",
      "        -1.195, -3.406,  0.454, -0.618, -0.628,  2.115,  1.002, -1.74 ,\n",
      "        -1.664, -1.041, -1.78 , -3.409,  0.457, -3.768, -0.24 ,  4.823,\n",
      "         0.67 ,  0.562, -0.564,  1.592,  0.037, -1.549, -0.824,  1.769,\n",
      "        -0.098,  2.63 ],\n",
      "       [ 1.728,  3.018, -1.144, -1.824,  2.884, -0.688, -2.135,  0.937,\n",
      "        -2.821, -3.895,  0.441, -1.048,  2.092,  0.355, -3.642, -2.721,\n",
      "        -1.894, -0.423,  1.024, -1.65 , -2.762, -0.558,  1.396, -0.348,\n",
      "        -2.319,  0.184,  2.365, -2.641, -2.09 , -1.779, -0.354,  0.181,\n",
      "        -0.851, -0.711, -4.205,  0.137, -1.012, -1.07 , -0.361, -1.207,\n",
      "         0.772, -2.405,  0.608,  2.842,  0.206, -0.276,  0.609, -4.511,\n",
      "         3.972, -1.259]])]\n"
     ]
    }
   ],
   "source": [
    "print(net.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9576 / 10000\n",
      "Epoch 1: 9576 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.fit(tr_data, 2, 10, 0.5, test_data=ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9576\n"
     ]
    }
   ],
   "source": [
    "print(net.evaluate(ts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.879,  0.151, -2.478, ..., -0.148,  0.677,  1.367],\n",
      "       [ 0.547, -1.459,  0.67 , ..., -0.01 ,  1.029,  0.353],\n",
      "       [-0.405, -1.009,  1.702, ...,  0.945, -0.199,  1.588],\n",
      "       ...,\n",
      "       [ 0.085, -0.593,  0.46 , ...,  1.29 , -0.549,  1.912],\n",
      "       [ 1.272,  0.442,  0.368, ..., -0.722, -0.513,  0.59 ],\n",
      "       [-0.228,  0.496, -2.584, ...,  0.111,  0.659,  0.191]]), array([[-0.894,  1.909,  0.373, -1.977, -2.366,  0.786, -2.839,  0.774,\n",
      "         0.287,  0.474, -3.459, -1.493,  0.04 ,  3.796,  1.13 ,  0.18 ,\n",
      "        -0.681, -4.29 , -0.13 , -1.832,  1.045, -2.019,  2.232, -3.315,\n",
      "         0.771, -1.56 , -0.556,  2.808, -2.843, -1.633,  2.965,  2.943,\n",
      "         0.414,  1.122, -1.206,  2.011,  1.021, -2.253,  0.517,  2.427,\n",
      "        -2.695, -0.379,  0.939, -2.051,  2.217, -3.399,  0.235, -1.732,\n",
      "        -1.052, -2.56 ],\n",
      "       [-0.885, -1.468,  0.936, -2.546, -3.375, -2.403, -0.447,  1.469,\n",
      "         0.427, -2.852, -0.766, -1.904,  1.685, -1.406, -2.69 ,  2.292,\n",
      "         2.045,  1.009, -1.77 , -0.806,  2.309,  1.269,  0.865, -1.071,\n",
      "        -0.23 ,  2.644, -0.304,  0.37 , -0.182, -0.366, -1.197, -0.397,\n",
      "         1.26 , -1.356, -1.182, -2.157, -3.19 ,  2.44 ,  2.563, -2.124,\n",
      "        -3.854,  2.682,  1.396, -0.281,  1.374,  0.968,  0.104,  1.189,\n",
      "        -0.679,  0.658],\n",
      "       [ 0.603, -0.913, -1.243,  0.617, -1.424,  0.549, -4.119, -0.021,\n",
      "        -3.208,  1.055, -1.887, -3.165, -3.656, -3.086,  1.734,  2.305,\n",
      "         2.81 , -2.029,  2.934,  2.338, -1.303, -0.034,  0.637, -3.509,\n",
      "        -0.834, -0.646, -0.526, -0.366, -1.685,  4.855, -1.071, -3.339,\n",
      "         3.386,  0.306,  2.338,  2.781, -0.186, -4.81 ,  0.119,  2.91 ,\n",
      "        -1.508, -1.669,  2.67 , -0.565, -2.023,  1.913, -1.051,  1.861,\n",
      "        -0.451,  1.195],\n",
      "       [ 1.392,  0.617,  0.808,  1.571, -3.524,  0.286, -1.391, -0.62 ,\n",
      "         0.581, -0.404, -2.645, -3.006,  2.97 , -2.472,  1.033,  0.717,\n",
      "        -3.728, -3.457, -2.92 ,  2.652, -0.073, -2.001, -0.098, -5.798,\n",
      "        -0.493,  3.809,  1.46 ,  2.539,  0.49 ,  3.089, -1.853,  0.633,\n",
      "        -2.801,  0.364, -1.739, -1.873, -1.506, -0.406, -1.133, -3.26 ,\n",
      "         3.11 , -0.86 , -2.415, -0.672, -2.191,  0.567,  1.842, -0.639,\n",
      "        -0.522,  0.57 ],\n",
      "       [-2.675, -4.559, -0.53 ,  0.128,  2.211,  1.239,  1.519, -0.676,\n",
      "        -1.144, -0.795, -2.907, -0.941, -0.937,  0.725, -2.02 , -0.79 ,\n",
      "        -1.685,  0.831,  0.08 , -0.74 , -0.62 , -2.223, -0.955, -2.166,\n",
      "        -0.277,  0.603, -2.618, -1.383,  0.422, -0.746, -2.074,  0.076,\n",
      "        -2.851,  1.556,  3.127, -1.157,  0.901,  1.901,  1.514, -0.2  ,\n",
      "        -2.109,  3.407, -0.217,  0.345,  2.255, -4.439,  0.867, -2.03 ,\n",
      "        -4.787, -0.528],\n",
      "       [-1.697,  1.09 , -0.69 ,  1.66 , -1.15 , -3.74 , -1.104, -3.892,\n",
      "        -1.088,  1.009,  4.937, -2.523,  1.896, -4.337, -0.337,  0.04 ,\n",
      "        -0.771,  1.988,  1.314, -1.417,  2.224,  1.406, -1.609,  5.405,\n",
      "        -0.301, -4.656,  0.915,  7.134,  1.695, -1.91 , -0.68 , -2.41 ,\n",
      "         3.415,  0.135,  0.238, -0.105,  2.183,  2.414, -1.26 , -3.233,\n",
      "        -2.025, -0.934,  0.36 , -0.106, -2.353,  0.275,  0.424,  0.698,\n",
      "         0.528, -1.561],\n",
      "       [ 0.716, -3.215,  1.937, -0.361, -1.892,  3.905, -2.194,  1.323,\n",
      "         1.017, -1.188,  2.562, -3.406, -0.865, -3.629, -2.803, -0.788,\n",
      "        -1.247, -0.888, -0.53 , -4.131, -1.393,  0.956, -2.596,  1.109,\n",
      "         2.567, -1.353, -0.673, -0.748,  0.871, -1.859,  2.258,  3.262,\n",
      "        -1.858, -3.195,  1.376,  1.57 ,  0.457, -0.633,  1.083,  3.321,\n",
      "        -0.438, -0.109, -0.584,  0.921, -0.204,  2.907,  0.047, -0.274,\n",
      "        -2.023, -3.279],\n",
      "       [-0.851,  0.663,  0.842, -2.287,  5.279, -1.663, -1.062,  0.5  ,\n",
      "         1.336,  2.28 , -1.328, -0.515, -2.038, -1.745,  2.7  ,  1.661,\n",
      "         2.722,  0.285, -3.302,  1.646,  2.543,  1.469, -0.784, -1.661,\n",
      "        -0.617, -0.686, -2.229, -3.422,  2.205, -0.003, -1.157, -1.44 ,\n",
      "         1.704, -0.569, -1.46 ,  0.83 ,  0.488,  1.861, -2.466,  1.549,\n",
      "        -0.215, -0.496,  0.02 , -3.128, -3.001, -4.358, -0.516, -2.394,\n",
      "        -0.357,  0.744],\n",
      "       [ 1.187,  0.291, -0.348, -0.556, -4.894, -3.147, -1.821, -2.246,\n",
      "         1.478, -0.664, -2.483, -2.941, -3.314, -0.498, -2.126, -4.731,\n",
      "        -2.077,  0.498,  0.146, -3.996, -4.417, -1.901,  0.556, -3.47 ,\n",
      "        -1.199, -3.338,  0.423, -0.614, -0.64 ,  2.194,  1.097, -1.762,\n",
      "        -1.665, -1.092, -1.746, -3.487,  0.361, -3.793, -0.209,  4.878,\n",
      "         0.673,  0.609, -0.473,  1.628, -0.005, -1.538, -0.831,  1.742,\n",
      "        -0.075,  2.593],\n",
      "       [ 1.768,  3.113, -1.257, -1.925,  2.73 , -0.76 , -2.173,  0.793,\n",
      "        -2.833, -3.914,  0.462, -1.096,  2.086,  0.385, -3.519, -2.668,\n",
      "        -1.917, -0.514,  0.978, -1.64 , -2.775, -0.55 ,  1.417, -0.374,\n",
      "        -2.314,  0.129,  2.438, -2.659, -2.118, -1.838, -0.292,  0.181,\n",
      "        -0.884, -0.696, -4.252,  0.226, -1.083, -1.103, -0.382, -1.292,\n",
      "         0.711, -2.523,  0.545,  2.81 ,  0.19 , -0.234,  0.633, -4.496,\n",
      "         4.13 , -1.232]])]\n"
     ]
    }
   ],
   "source": [
    "print(net.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ba86c1f12825e365d050cd954c0834b0c56a4483f355d89046a9439c3fc83b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tf1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
